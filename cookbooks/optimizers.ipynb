{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abcom/.pyenv/versions/3.11.0/envs/reviewturtl/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/abcom/Desktop/github/reviewturtl\")\n",
    "import pandas as pd\n",
    "from reviewturtl.settings import initialize_dspy_with_configs\n",
    "initialize_dspy_with_configs()\n",
    "csv_file_path = 'summarizer_signature_examples.csv'\n",
    "df = pd.read_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dspy.teleprompt import MIPRO\n",
    "# from reviewturtl.settings import get_4o_token_model\n",
    "# teleprompter = MIPRO(prompt_model=get_4o_token_model(), task_model=get_4o_token_model(), metric=SummarizerMetricwithgt, num_candidates=3, init_temperature=0.5)\n",
    "# kwargs = dict(num_threads=1, display_progress=True, display_table=0)\n",
    "# MIPRO_compiled = teleprompter.compile(pg, trainset=trainset, num_trials=3, max_bootstrapped_demos=1, max_labeled_demos=0, eval_kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m\u001b[1mWARNING: MIPRO has been deprecated and replaced with MIPROv2.  MIPRO will be removed in a future release. \u001b[0m\n",
      "\u001b[93m\u001b[1mWARNING: Projected Language Model (LM) Calls\u001b[0m\n",
      "\n",
      "Please be advised that based on the parameters you have set, the maximum number of LM calls is projected as follows:\n",
      "\n",
      "\u001b[93m- Task Model: \u001b[94m\u001b[1m3\u001b[0m\u001b[93m examples in dev set * \u001b[94m\u001b[1m3\u001b[0m\u001b[93m trials * \u001b[94m\u001b[1m# of LM calls in your program\u001b[0m\u001b[93m = (\u001b[94m\u001b[1m9 * # of LM calls in your program\u001b[0m\u001b[93m) task model calls\u001b[0m\n",
      "\u001b[93m- Prompt Model: # data summarizer calls (max \u001b[94m\u001b[1m10\u001b[0m\u001b[93m) + \u001b[94m\u001b[1m3\u001b[0m\u001b[93m * \u001b[94m\u001b[1m1\u001b[0m\u001b[93m lm calls in program = \u001b[94m\u001b[1m13\u001b[0m\u001b[93m prompt model calls\u001b[0m\n",
      "\n",
      "\u001b[93m\u001b[1mEstimated Cost Calculation:\u001b[0m\n",
      "\n",
      "\u001b[93mTotal Cost = (Number of calls to task model * (Avg Input Token Length per Call * Task Model Price per Input Token + Avg Output Token Length per Call * Task Model Price per Output Token) \n",
      "            + (Number of calls to prompt model * (Avg Input Token Length per Call * Task Prompt Price per Input Token + Avg Output Token Length per Call * Prompt Model Price per Output Token).\u001b[0m\n",
      "\n",
      "For a preliminary estimate of potential costs, we recommend you perform your own calculations based on the task\n",
      "and prompt models you intend to use. If the projected costs exceed your budget or expectations, you may consider:\n",
      "\n",
      "\u001b[93m- Reducing the number of trials (`num_trials`), the size of the trainset, or the number of LM calls in your program.\u001b[0m\n",
      "\u001b[93m- Using a cheaper task model to optimize the prompt.\u001b[0m\n",
      "To proceed with the execution of this program, please confirm by typing \u001b[94m'y'\u001b[0m for yes or \u001b[94m'n'\u001b[0m for no.\n",
      "\n",
      "If you would like to bypass this confirmation step in future executions, set the \u001b[93m`requires_permission_to_run`\u001b[0m flag to \u001b[93m`False` when calling compile.\u001b[0m\n",
      "\n",
      "\u001b[93mAwaiting your input...\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:00<00:00, 14.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:00<00:00, 97.91it/s]\n",
      "[I 2024-07-29 03:50:57,679] A new study created in memory with name: no-name-ae152e30-6f65-40b2-8469-d948a93ca297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n",
      "Starting trial #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.0 / 3  (433.3): 100%|██████████| 3/3 [00:00<00:00, 331.71it/s]\n",
      "[I 2024-07-29 03:50:57,716] Trial 0 finished with value: 433.33 and parameters: {'5787184592_predictor_instruction': 1, '5787184592_predictor_demos': 0}. Best is trial 0 with value: 433.33.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.5 / 3  (450.0): 100%|██████████| 3/3 [00:00<00:00, 920.01it/s]\n",
      "[I 2024-07-29 03:50:57,754] Trial 1 finished with value: 450.0 and parameters: {'5787184592_predictor_instruction': 1, '5787184592_predictor_demos': 2}. Best is trial 1 with value: 450.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.5 / 3  (450.0): 100%|██████████| 3/3 [00:00<00:00, 2979.61it/s]\n",
      "[I 2024-07-29 03:50:57,787] Trial 2 finished with value: 450.0 and parameters: {'5787184592_predictor_instruction': 0, '5787184592_predictor_demos': 2}. Best is trial 1 with value: 450.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning predictor.predictor = Predict(StringSignature(file_diff -> reasoning, walkthrough, changes_in_tabular_description\n",
      "    instructions='Proposed Instruction: \\n\\n**Task Overview:**\\nYou are provided with a `file_diff` representing changes made to Python files related to geometric computations. Your task is to analyze these changes and generate a structured summary that includes a `walkthrough` and a `changes_in_tabular_description`.\\n\\n**Instructions:**\\n\\n1. **Analyze the File Diff:**\\n   - Identify the file(s) that have been modified, added, or deleted.\\n   - Note the specific changes made within each file, focusing on modifications to mathematical computations and constants.\\n\\n2. **Generate the Walkthrough:**\\n   - Provide a concise explanation of the overall changes made to the file(s).\\n   - Highlight the purpose and impact of these changes, especially in terms of how they affect the computation of geometric properties.\\n\\n3. **Create the Changes in Tabular Description:**\\n   - Summarize the changes for each file in a markdown table format.\\n   - The table should have two columns: `File Name` and `Changes`.\\n   - Ensure that the description in the `Changes` column is brief and to the point, focusing on the key modifications.\\n\\n**Formatting Guidelines:**\\n- The walkthrough should be a short paragraph summarizing the changes.\\n- The tabular description should be formatted in markdown and should not mention specific line numbers.\\n- Use clear and concise language suitable for developers or learners who need to understand the code changes quickly.\\n\\n**Example Output:**\\n\\n```\\nwalkthrough: The calculation of the area of the circle has been updated to use a newly defined constant `PI` instead of `math.pi`.\\n\\nchanges_in_tabular_description: \\n| File Name | Changes |\\n| --------- | -------- |\\n| circle.py | Introduced a new constant `PI` and updated the area calculation to use this constant instead of `math.pi`. |\\n```'\n",
      "    file_diff = Field(annotation=str required=True json_schema_extra={'desc': 'The diff of the file', '__dspy_field_type': 'input', 'prefix': 'File Diff:'})\n",
      "    reasoning = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${produce the walkthrough, changes_in_tabular_description}. We ...', '__dspy_field_type': 'output'})\n",
      "    walkthrough = Field(annotation=str required=True json_schema_extra={'desc': 'The summary of changes made in the PR', '__dspy_field_type': 'output', 'prefix': 'Walkthrough:'})\n",
      "    changes_in_tabular_description = Field(annotation=str required=True json_schema_extra={'desc': 'The Markdown table containing the changes in the file', '__dspy_field_type': 'output', 'prefix': '```\\nwalkthrough: \\n```'})\n",
      ")) from continue_program\n",
      "[('predictor.predictor', Predict(StringSignature(file_diff -> reasoning, walkthrough, changes_in_tabular_description\n",
      "    instructions='Proposed Instruction: \\n\\n**Task Overview:**\\nYou are provided with a `file_diff` representing changes made to Python files related to geometric computations. Your task is to analyze these changes and generate a structured summary that includes a `walkthrough` and a `changes_in_tabular_description`.\\n\\n**Instructions:**\\n\\n1. **Analyze the File Diff:**\\n   - Identify the file(s) that have been modified, added, or deleted.\\n   - Note the specific changes made within each file, focusing on modifications to mathematical computations and constants.\\n\\n2. **Generate the Walkthrough:**\\n   - Provide a concise explanation of the overall changes made to the file(s).\\n   - Highlight the purpose and impact of these changes, especially in terms of how they affect the computation of geometric properties.\\n\\n3. **Create the Changes in Tabular Description:**\\n   - Summarize the changes for each file in a markdown table format.\\n   - The table should have two columns: `File Name` and `Changes`.\\n   - Ensure that the description in the `Changes` column is brief and to the point, focusing on the key modifications.\\n\\n**Formatting Guidelines:**\\n- The walkthrough should be a short paragraph summarizing the changes.\\n- The tabular description should be formatted in markdown and should not mention specific line numbers.\\n- Use clear and concise language suitable for developers or learners who need to understand the code changes quickly.\\n\\n**Example Output:**\\n\\n```\\nwalkthrough: The calculation of the area of the circle has been updated to use a newly defined constant `PI` instead of `math.pi`.\\n\\nchanges_in_tabular_description: \\n| File Name | Changes |\\n| --------- | -------- |\\n| circle.py | Introduced a new constant `PI` and updated the area calculation to use this constant instead of `math.pi`. |\\n```'\n",
      "    file_diff = Field(annotation=str required=True json_schema_extra={'desc': 'The diff of the file', '__dspy_field_type': 'input', 'prefix': 'File Diff:'})\n",
      "    reasoning = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${produce the walkthrough, changes_in_tabular_description}. We ...', '__dspy_field_type': 'output'})\n",
      "    walkthrough = Field(annotation=str required=True json_schema_extra={'desc': 'The summary of changes made in the PR', '__dspy_field_type': 'output', 'prefix': 'Walkthrough:'})\n",
      "    changes_in_tabular_description = Field(annotation=str required=True json_schema_extra={'desc': 'The Markdown table containing the changes in the file', '__dspy_field_type': 'output', 'prefix': '```\\nwalkthrough: \\n```'})\n",
      ")))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictor.predictor = Predict(StringSignature(file_diff -> reasoning, walkthrough, changes_in_tabular_description\n",
       "    instructions='Proposed Instruction: \\n\\n**Task Overview:**\\nYou are provided with a `file_diff` representing changes made to Python files related to geometric computations. Your task is to analyze these changes and generate a structured summary that includes a `walkthrough` and a `changes_in_tabular_description`.\\n\\n**Instructions:**\\n\\n1. **Analyze the File Diff:**\\n   - Identify the file(s) that have been modified, added, or deleted.\\n   - Note the specific changes made within each file, focusing on modifications to mathematical computations and constants.\\n\\n2. **Generate the Walkthrough:**\\n   - Provide a concise explanation of the overall changes made to the file(s).\\n   - Highlight the purpose and impact of these changes, especially in terms of how they affect the computation of geometric properties.\\n\\n3. **Create the Changes in Tabular Description:**\\n   - Summarize the changes for each file in a markdown table format.\\n   - The table should have two columns: `File Name` and `Changes`.\\n   - Ensure that the description in the `Changes` column is brief and to the point, focusing on the key modifications.\\n\\n**Formatting Guidelines:**\\n- The walkthrough should be a short paragraph summarizing the changes.\\n- The tabular description should be formatted in markdown and should not mention specific line numbers.\\n- Use clear and concise language suitable for developers or learners who need to understand the code changes quickly.\\n\\n**Example Output:**\\n\\n```\\nwalkthrough: The calculation of the area of the circle has been updated to use a newly defined constant `PI` instead of `math.pi`.\\n\\nchanges_in_tabular_description: \\n| File Name | Changes |\\n| --------- | -------- |\\n| circle.py | Introduced a new constant `PI` and updated the area calculation to use this constant instead of `math.pi`. |\\n```'\n",
       "    file_diff = Field(annotation=str required=True json_schema_extra={'desc': 'The diff of the file', '__dspy_field_type': 'input', 'prefix': 'File Diff:'})\n",
       "    reasoning = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${produce the walkthrough, changes_in_tabular_description}. We ...', '__dspy_field_type': 'output'})\n",
       "    walkthrough = Field(annotation=str required=True json_schema_extra={'desc': 'The summary of changes made in the PR', '__dspy_field_type': 'output', 'prefix': 'Walkthrough:'})\n",
       "    changes_in_tabular_description = Field(annotation=str required=True json_schema_extra={'desc': 'The Markdown table containing the changes in the file', '__dspy_field_type': 'output', 'prefix': '```\\nwalkthrough: \\n```'})\n",
       "))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from reviewturtl.src.optimizers.summary_optimizer import SummaryOptimizer\n",
    "optimizer_configs = {\n",
    "                \"num_trials\": 3,\n",
    "                \"max_bootstrapped_demos\": 1,\n",
    "                \"max_labeled_demos\": 0,\n",
    "                \"number_of_candidates\": 3,\n",
    "            }\n",
    "summary_optimizer = SummaryOptimizer(csv_path=\"summarizer_signature_examples.csv\",save_path=\"/Users/abcom/Desktop/github/reviewturtl/reviewturtl/src/programmes/compiled_programmes\")\n",
    "summary_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reviewturtl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
